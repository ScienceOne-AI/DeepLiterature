# encoding: utf-8
import time
import json
import re
import traceback

from agents import OrchestratorAgent, OptimizerAgent, SelectorAgent, SufficiencyValidatorAgent, ReorchestratorAgent, AssitantAgent, CodeAgent
from tools.executors import CodeExecutor
from llms import LLMFactory
from utils.common_utils import latex_render, replace_ref_tag2md, get_location_by_ip, get_real_time_str
from utils.logger import model_logger
import uuid

from .utils import run_code, fetch_search_result
from .utils import function_call_receive_code_results, step_by_step_process, process_message
from .utils import function_call_receive_document, function_call_receive_snippet, function_call_sent, generate_call_id, get_tool_list, remove_ref_tag, update_search_res
from .utils import convert_think_message_to_markdown, format_data
from config import LANGUAGE, SEARCH_ENGINE, LLM_MODEL

def run(question, queue, api_queue, lang, context=dict(), save_jsonl_path="", debug_verbose=True, meta_verbose=False):
    try:
        run_throw_exception(question, queue, api_queue, lang, context=context, save_jsonl_path=save_jsonl_path,  debug_verbose=debug_verbose, meta_verbose=meta_verbose)
    except Exception as e:
        traceback.print_exc()
        queue.put(["exception", None])
        api_queue.put("exception")
    return

def run_throw_exception(question, queue, api_queue, lang, context=dict(), save_jsonl_path="", debug_verbose=True, meta_verbose=False):
    llm = LLMFactory.construct(LLM_MODEL)
    orchestrator = OrchestratorAgent(llm=llm, lang=lang)
    optimizer = OptimizerAgent(llm=llm, lang=lang)
    selector = SelectorAgent(llm=llm, lang=lang)
    validator = SufficiencyValidatorAgent(llm=llm, lang=lang)
    reorchestrator = ReorchestratorAgent(llm=llm, lang=lang)
    assitant = AssitantAgent(llm=llm, lang=lang)
    code_agent = CodeAgent(llm=llm, lang=lang)

    code_executor = CodeExecutor()

    request_id = str(uuid.uuid4())
    now_messages = []
    meta_data = {
        "time_stamp":f"{get_real_time_str()}",
        "question": context['question'],
        "feedback": None,
        "messages" : now_messages
    }
    context["question"] = question
    queue.put(["bar", "ðŸ–¥ï¸ æ™ºèƒ½ä½“ Â· è·¯å¾„è§„åˆ’"])
    context["online_steps"].append("ðŸ–¥ï¸ æ™ºèƒ½ä½“ Â· è·¯å¾„è§„åˆ’")
    result = dict()
    queue.put(["placeholder_begin", None])
    context["online_steps"].append("")
    _reasoning_content = ""
    for label, _stream_text in orchestrator.run(question):
        if label == "think":
            _reasoning_content += _stream_text
            queue.put(["placeholder_think_stream_markdown", convert_think_message_to_markdown(_reasoning_content)])
            context["online_steps"][-1] = convert_think_message_to_markdown(_reasoning_content)
            api_queue.put(format_data(reasoning_content=_stream_text, id=request_id, stage="orchestrating"))
        else:
            result = _stream_text
    now_messages.append({"reasoning_content": _reasoning_content, "content": _stream_text, "role": "assistant", "stage": "orchestrating"})
    context["if_search"] = result['if_search']
    context["if_code"] = result['if_code']
    context["search_type"] = result['search_type']
    context["search_process"] = result['search_process']

    raw_infos = []
    all_steps = step_by_step_process(context['search_process'])
    raw_info = [[] for _ in range(len(all_steps))]
    raw_infos.append(raw_info)

    context['raw_info'] = raw_infos
    context['search_res'] = raw_infos
    context['llm_answer'] = None


    if context['if_search'] == "å¦" and context['if_code'] == 'å¦' or LANGUAGE == "en" and str(context['if_search']).lower() == "no" and str(context['if_code']).lower() == 'no':
        if debug_verbose:
            queue.put(["bar", ":grey[ðŸ«¢ æ™ºèƒ½ä½“ Â· å½“å‰é—®é¢˜æ— éœ€ Function]"])
            context["online_steps"].append(":grey[ðŸ«¢ æ™ºèƒ½ä½“ Â· å½“å‰é—®é¢˜æ— éœ€ Function]")
            start_time = time.time()
            llm_answer = ""
            for label, _stream_text in llm.stream_chat(user_content=question):
                if label == "answer":
                    llm_answer += _stream_text
                    api_queue.put(format_data(content=_stream_text, id=request_id, stage="answering without function calling"))
                else:
                    result = _stream_text
            context["online_answer"] = llm_answer
            now_messages.append({"content": context["online_answer"], "role": "assistant", "stage": "answering without function calling"})
            in_out = {"state": "model_answer", "input": question, "output": context["online_answer"], "cost_time": str(round(time.time()-start_time, 3))}
            model_logger.info(json.dumps(in_out, ensure_ascii=False))
    else:
        all_evidences= [] # å°†æ‰€æœ‰çš„è¯æ®ï¼ˆæ–‡æœ¬ï¼‰ä¿¡æ¯éƒ½è®°å½•ä¸‹æ¥
        code_evidences = [] # å°†æ‰€æœ‰ä»£ç è¯æ®(æ–‡æœ¬)ä¿¡æ¯éƒ½è®°å½•ä¸‹æ¥
        code_results_dict_list = [] # æŠŠä»£ç æœ€åŽŸå§‹çš„è®¡ç®—ç»“æžœä¿å­˜ä¸‹æ¥
        empty_search = True
        empty_code = True
        use_tool_record = set()
        process_pipeline = ['system', 'user','step_by_step', 'assistant']
        for now_stage in process_pipeline:
            if now_stage == 'system':
                now_messages.append(process_message(f"You are a helpful assistant. å½“å‰çš„æ—¶é—´ä¸º{get_real_time_str()}ã€‚å½“å‰æ‰€åœ¨åœ°: {get_location_by_ip()}ã€‚"))
            elif now_stage == 'user':
                now_messages.append(process_message(content = context['question'], role = "user"))
            elif now_stage == 'step_by_step':
                # è¿›è¡Œè·¯å¾„çš„ä¼˜åŒ–è¿‡ç¨‹
                queue.put(["bar", "âœï¸ æ™ºèƒ½ä½“ Â· è·¯å¾„ä¼˜åŒ–"])
                context["online_steps"].append("âœï¸ æ™ºèƒ½ä½“ Â· è·¯å¾„ä¼˜åŒ–")
                queue.put(["placeholder_begin", None])
                context["online_steps"].append("")
                optimize_search_process_text = ""
                _reasoning_content = ""
                for label, _stream_text in optimizer.run(context['search_process']):
                    if label == "think":
                        _reasoning_content += _stream_text
                        queue.put(["placeholder_think_stream_markdown", convert_think_message_to_markdown(_reasoning_content)])
                        context["online_steps"][-1] = convert_think_message_to_markdown(_reasoning_content)
                        api_queue.put(format_data(reasoning_content=_stream_text, id=request_id, stage="optimize search process"))
                    else:
                        optimize_search_process_text = _stream_text
                now_messages.append({"reasoning_content": _reasoning_content, "content": optimize_search_process_text, "role": "assistant", "stage": "optimize search process"})
                if len(optimize_search_process_text) != 0:
                    context['search_process'] = optimize_search_process_text


                all_steps = step_by_step_process(context['search_process'])
                latest_search_process = context['search_process'] # è®°å½•ä¸Šä¸€æ­¥çš„search_process
                for step_idx, step in enumerate(all_steps):
                    if "webSearch" in all_steps[step_idx]['step_desc']:                      
                        now_step_keywords = all_steps[step_idx]['now_step_keywords']
                        if len(now_step_keywords) == 0:
                            break

                        use_tool_record.add("webSearch")
                        

                        empty_search, new_raw_info = fetch_search_result(all_steps, step_idx, context['raw_info'], debug_verbose, search_type=SEARCH_ENGINE, context=context, queue=queue)
                        context['raw_info'] = new_raw_info  

                        if empty_search:
                            if debug_verbose:
                                queue.put(["error", "å½“å‰è®¿é—®äººæ•°è¿‡å¤šï¼Œç½‘é¡µè¢«æŒ¤çˆ†å•¦ï¼Œè¯·ç¨åŽå°è¯•..."])
                            break

                        # æ›´æ–° search_res (æœªä¸‹è½½æ­£æ–‡)
                        new_search_res = update_search_res(all_steps, step_idx, context['search_res'], context['raw_info'], debug_verbose, context=context)
                        context['search_res'] = new_search_res

                        now_step_keywords = all_steps[step_idx]['now_step_keywords']
                        now_step_keyword_idxs = all_steps[step_idx]['now_step_keyword_idxs']
                        argument_values_list = [[now_step_keyword] for now_step_keyword in now_step_keywords]
                        call_ids = [generate_call_id() for _ in range(len(now_step_keywords))]
                        
                        # æ•´åˆå…³é”®è¯å’Œå¯¹åº”çš„æœç´¢ç»“æžœ
                        now_message = function_call_sent(argument_values_list, call_ids, "webSearch")
                        now_messages.append(now_message)
                        api_queue.put(format_data(reasoning_content="", id=request_id, stage="web search", tool_calls=now_message["tool_calls"]))
                        now_search_res = []
                        now_raw_info = []
                        for now_step_keyword_idx in now_step_keyword_idxs:
                            now_search_res.append(context['search_res'][now_step_keyword_idx])
                            now_raw_info.append(context['raw_info'][now_step_keyword_idx])
                        now_message = function_call_receive_snippet(now_step_keywords, call_ids, now_search_res, now_raw_info)
                        now_messages.extend(now_message)
                        api_queue.put(format_data(content=now_message, id=request_id, stage="web search"))

                        idx_list = [] # è®°å½•æ‰€æœ‰éœ€è¦ URL è§£æžçš„æ–‡ç«  idx
                        now_content = "" # CoT è¿‡ç¨‹
                        now_evidence = [] # è®°å½•å½“å‰æ‹¥æœ‰çš„æ‰€æœ‰è¯æ®ä¿¡æ¯
                        snippet_list = []

                        # ç­›é€‰ç›¸å…³çš„ç½‘é¡µå¿«ç…§
                        for keyword_idx, now_search in enumerate(now_search_res):
                            for doc_info in now_search:
                                now_title = doc_info['title']
                                now_snippet = doc_info['snippet'].replace("\n", " ").replace("\r","")
                                if len(now_snippet) == 0:
                                    now_snippet = doc_info['title']
                                
                                snippet_list.append(f"idx:{doc_info['idx']} snippet `{now_snippet}`")

                        choose_snippet_list = []
                        queue.put(["bar", "ðŸ”¬ æ™ºèƒ½ä½“ Â· ç­›é€‰ç›¸å…³ç½‘é¡µ"])
                        context["online_steps"].append("ðŸ”¬ æ™ºèƒ½ä½“ Â· ç­›é€‰ç›¸å…³ç½‘é¡µ")
                        queue.put(["placeholder_begin", None])
                        context["online_steps"].append("")
                        _reasoning_content = ""
                        for label, _stream_text in selector.run(all_steps[step_idx]['step_desc'], snippet_list):
                            if label == "think":
                                _reasoning_content += _stream_text
                                queue.put(["placeholder_think_stream_markdown", convert_think_message_to_markdown(_reasoning_content)])
                                context["online_steps"][-1] = convert_think_message_to_markdown(_reasoning_content)
                                api_queue.put(format_data(reasoning_content=_stream_text, id=request_id, stage="determine relevance"))
                            else:
                                choose_snippet_list = _stream_text
                        api_queue.put(format_data(content=choose_snippet_list, id=request_id, stage="determine relevance"))
                        now_messages.append({"reasoning_content": _reasoning_content, "content": choose_snippet_list, "role": "assistant", "stage": "determine relevance"})
                        new_snippet_list = []
                        if debug_verbose:
                            render_text = ""
                            for choose_snippet_idx in choose_snippet_list:
                                render_text += ":grey-background[" + "ðŸ”—" + str(choose_snippet_idx + 1) +"] "
                            if len(render_text) == 0:
                                render_text = ":grey-background[æ— ]"
                            queue.put(["bar", "ðŸ¤” æ™ºèƒ½ä½“ Â· å’Œé—®é¢˜ç›¸å…³çš„ç½‘é¡µæœ‰ï¼š" + render_text])
                            context["online_steps"].append("ðŸ¤” æ™ºèƒ½ä½“ Â· å’Œé—®é¢˜ç›¸å…³çš„ç½‘é¡µæœ‰ï¼š" + render_text)
                            
                        for keyword_idx, now_search in enumerate(now_search_res):
                            for doc_info in now_search:
                                if doc_info['idx'] in choose_snippet_list:
                                    now_snippet = doc_info['snippet'].replace("\n", " ").replace("\r","")
                                    now_evidence.append({doc_info['idx']:f"{doc_info['title']}\nsnippet:{now_snippet}\nlink:{doc_info['url']}"})
                                    new_snippet_list.append(f"idx:{doc_info['idx']} snippet `{now_snippet}`")
                        snippet_list = new_snippet_list
                        
                        # åˆ¤æ–­å½“å‰ç½‘é¡µå¿«ç…§ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿ
                        if len(snippet_list) > 0: 
                            response = dict()
                            queue.put(["bar", "ðŸ”¬ æ™ºèƒ½ä½“ Â· åˆ¤æ–­ç½‘é¡µæ•°æ®æ˜¯å¦è¶³å¤Ÿ"])
                            context["online_steps"].append("ðŸ”¬ æ™ºèƒ½ä½“ Â· åˆ¤æ–­ç½‘é¡µæ•°æ®æ˜¯å¦è¶³å¤Ÿ")
                            queue.put(["placeholder_begin", None])
                            context["online_steps"].append("")
                            _reasoning_content = ""
                            for label, _stream_text in validator.run(all_steps[step_idx]['step_desc'], "\n".join(snippet_list)):
                                if label == "think":
                                    _reasoning_content += _stream_text
                                    queue.put(["placeholder_think_stream_markdown", convert_think_message_to_markdown(_reasoning_content)])
                                    context["online_steps"][-1] = convert_think_message_to_markdown(_reasoning_content)
                                    api_queue.put(format_data(reasoning_content=_stream_text, id=request_id, stage="sufficency validating"))
                                else:
                                    response = _stream_text
                            now_messages.append({"reasoning_content": _reasoning_content, "content": response, "role": "assistant", "stage": "sufficency validating"})
                            now_content += response['analysis_sentence'] + "\n"
                            now_content += response['trans_sentence'] + "\n"
                            for snippet_sentence in response['snippet_sentence']:
                                now_content += snippet_sentence[2:] + "\n"
                                match = re.search(r'idx:(\d+)', snippet_sentence)
                                if snippet_sentence[0] == 'Ã—':
                                    if match:
                                        idx_number = int(match.group(1))
                                        # è¯´æ˜Žå½“å‰ç½‘é¡µå¿«ç…§éœ€è¦è§£æžURLå†…å®¹
                                        idx_list.append(idx_number)
                                else:
                                    if match:
                                        idx_number = int(match.group(1))

                        # è§£æžURLå†…å®¹
                        if len(idx_list) != 0:
                            api_queue.put(format_data(content=idx_list, id=request_id, stage="sufficency validating"))
                            use_tool_record.add("mclick")
                            render_text = ""
                            for mclick_idx in idx_list:
                                render_text += ":blue-background[" + "ðŸ“ƒ" + str(mclick_idx + 1) +"] "
                            queue.put(["bar", ":grey[ðŸ˜® æ™ºèƒ½ä½“ Â· å½“å‰ç½‘é¡µçš„ä¿¡æ¯ä¼¼ä¹Žä¸å¤ªå¤Ÿï¼Œæ‰“å¼€ç½‘é¡µçœ‹ä¸€çœ‹...]"])
                            context["online_steps"].append(":grey[ðŸ˜® æ™ºèƒ½ä½“ Â· å½“å‰ç½‘é¡µçš„ä¿¡æ¯ä¼¼ä¹Žä¸å¤ªå¤Ÿï¼Œæ‰“å¼€ç½‘é¡µçœ‹ä¸€çœ‹...]")
                            
                            queue.put(["bar", "ðŸ¤¨ æ™ºèƒ½ä½“ Â· è¿™äº›ç½‘é¡µéœ€è¦æ‰“å¼€ï¼š" + render_text])
                            context["online_steps"].append("ðŸ¤¨ æ™ºèƒ½ä½“ Â· è¿™äº›ç½‘é¡µéœ€è¦æ‰“å¼€ï¼š" + render_text)

                            queue.put(["bar", "ðŸ“– è°ƒå·¥å…· Â· ç½‘é¡µé˜…è¯»"])
                            context["online_steps"].append("ðŸ“– è°ƒå·¥å…· Â· ç½‘é¡µé˜…è¯»")
                            argument_values_list = [idx_list]
                            call_ids = [generate_call_id()] 
                            # æ•´åˆURLå’Œå¯¹åº”çš„è§£æžå†…å®¹
                            now_message = function_call_sent([argument_values_list], call_ids, "mclick", ["idx_list"], now_content)
                            now_messages.append(now_message)
                            api_queue.put(format_data(reasoning_content="", id=request_id, stage="fetch page body", tool_calls=now_message["tool_calls"]))
                            now_message = function_call_receive_document(idx_list, call_ids, now_search_res, context=context, queue=queue)
                            api_queue.put(format_data(content=now_message, id=request_id, stage="fetch page body"))
                            now_messages.append(now_message)
                            document_list = json.loads(now_message['content'])

                            # æ›´æ–°evidenceï¼Œå°† document_list ä¸­å‡ºçŽ°çš„ï¼Œç”¨æ­£æ–‡ä¿¡æ¯ä»£æ›¿åŽŸæ¥è¯æ®å¥å¯¹åº”çš„ç½‘é¡µå¿«ç…§
                            for i, evi_dict in enumerate(now_evidence):
                                key1 = next(iter(evi_dict))
                                for doc_dict in document_list:
                                    key2 = next(iter(doc_dict))
                                    if str(key1) == str(key2):
                                        now_evidence[i][key1] = doc_dict[key2]
                        else:
                            queue.put(["bar","ðŸ˜ æ™ºèƒ½ä½“ Â· åˆ†æžå®Œæ¯•"])
                            context["online_steps"].append("ðŸ˜ æ™ºèƒ½ä½“ Â· åˆ†æžå®Œæ¯•")
                            queue.put(["divider", None])
                            context["online_steps"].append("\n---\n")
                        now_evidence_list = [json.dumps(evi_dict,ensure_ascii=False) for evi_dict in now_evidence]
                        all_evidences.extend(now_evidence_list)
                        if len(now_evidence_list) > 0:
                            # æ ¹æ®å½“å‰çš„çŠ¶æ€ä¿¡æ¯ï¼ˆå½“å‰evidenceã€å½“å‰æ­¥éª¤ã€æ•´ä½“æ­¥éª¤ï¼‰é‡æ–°è§„åˆ’è·¯å¾„
                            if step_idx != len(all_steps) - 1:
                                response = dict()
                                queue.put(["bar","ðŸ”¬ æ™ºèƒ½ä½“ Â· ä¾æ®å½“å‰çŠ¶æ€é‡æ–°è§„åˆ’è·¯å¾„"])
                                context["online_steps"].append("ðŸ”¬ æ™ºèƒ½ä½“ Â· ä¾æ®å½“å‰çŠ¶æ€é‡æ–°è§„åˆ’è·¯å¾„")
                                queue.put(["placeholder_begin", None])
                                context["online_steps"].append("")
                                _reasoning_content = ""
                                for label, _stream_text in reorchestrator.run(context['question'], now_evidence_list, all_steps[step_idx]['step_desc'], latest_search_process):
                                    if label == "think":
                                        _reasoning_content += _stream_text
                                        queue.put(["placeholder_think_stream_markdown", convert_think_message_to_markdown(_reasoning_content)])
                                        context["online_steps"][-1] = convert_think_message_to_markdown(_reasoning_content)
                                        api_queue.put(format_data(reasoning_content=_stream_text, id=request_id, stage="reorchestrate"))
                                    else:
                                        response = _stream_text
                                now_messages.append({"reasoning_content": _reasoning_content, "content": response, "role": "assistant", "stage": "reorchestrate"})
                                latest_search_process = response['update_whole_step']
                                all_steps = step_by_step_process(latest_search_process)
                                queue.put(["bar",f"ðŸ”¨æ›´æ–°ä¸ºæµç¨‹ï¼š{latest_search_process}"])
                                context["online_steps"].append(f"ðŸ”¨æ›´æ–°ä¸ºæµç¨‹ï¼š{latest_search_process}")
                                api_queue.put(format_data(content=latest_search_process, id=request_id, stage="reorchestrate"))
                            else:
                                context['search_process'] = latest_search_process
                                break
                        else:
                            start_index = latest_search_process.find("-> " + all_steps[step_idx]['step_desc']) 
                            latest_search_process = latest_search_process[:start_index] + "-> " + all_steps[step_idx]['step_desc']
                            all_steps = step_by_step_process(latest_search_process)
                            context['search_process'] = latest_search_process
                            break

                    else:
                        empty_code = False
                        use_tool_record.add("codeRunner")
                        now_step_keywords = all_steps[step_idx]['now_step_keywords']
                        if len(now_step_keywords) == 0:
                            if debug_verbose:
                                pass
                            continue
                        # æž„å»ºå‚è€ƒä¿¡æ¯
                        if step_idx == 0:
                            if LANGUAGE == "en":
                                previous_reference = "None"
                            else:
                                previous_reference = "æ— "
                        else:
                            previous_reference = all_steps[step_idx - 1]['step_desc'] # æŠŠä¸Šä¸€æ­¥ä½œä¸ºå‚è€ƒä¿¡æ¯
                        all_codes, all_code_results = run_code(code_agent, code_executor, meta_data,all_steps, step_idx, context['question'], previous_reference, all_evidences, debug_verbose, context=context, queue=queue, api_queue=api_queue, request_id=request_id)
                        code_results_dict_list.extend(all_code_results)
                        argument_values_list = [[all_code] for all_code in all_codes]
                        call_ids = [generate_call_id() for _ in range(len(now_step_keywords))]

                        # æ•´åˆç”Ÿæˆä»£ç å’Œä»£ç çš„æ‰§è¡Œç»“æžœ
                        now_message = function_call_sent(argument_values_list, call_ids, "codeRunner", argument_names = ['code'])
                        now_messages.append(now_message)
                        code_evidence, now_message = function_call_receive_code_results(all_steps[step_idx]['now_step_keywords'], all_codes, call_ids, all_code_results)
                        now_messages.extend(now_message)
                        code_evidences.extend(code_evidence)
                        
                        # æ ¹æ®å½“å‰çš„çŠ¶æ€ä¿¡æ¯ï¼ˆå½“å‰ä»£ç æ‰§è¡Œç»“æžœã€å½“å‰æ­¥éª¤ã€æ•´ä½“æ­¥éª¤ï¼‰é‡æ–°è§„åˆ’è·¯å¾„
                        if step_idx != len(all_steps) - 1:
                            all_code_results_list = []
                            for all_code_result in all_code_results:
                                all_code_results_list.append(str(all_code_result))
                            response = dict()
                            queue.put(["bar", "ðŸ”¬ æ™ºèƒ½ä½“ Â· ä¾æ®å½“å‰çŠ¶æ€é‡æ–°è§„åˆ’è·¯å¾„"])
                            context["online_steps"].append("ðŸ”¬ æ™ºèƒ½ä½“ Â· ä¾æ®å½“å‰çŠ¶æ€é‡æ–°è§„åˆ’è·¯å¾„")
                            queue.put(["placeholder_begin", None])
                            context["online_steps"].append("")
                            _reasoning_content = ""
                            for label, _stream_text in reorchestrator.run(context['question'], all_code_results_list, all_steps[step_idx]['step_desc'], latest_search_process):
                                if label == "think":
                                    _reasoning_content += _stream_text
                                    queue.put(["placeholder_think_stream_markdown", convert_think_message_to_markdown(_reasoning_content)])
                                    context["online_steps"][-1] = convert_think_message_to_markdown(_reasoning_content)
                                    api_queue.put(format_data(reasoning_content=_stream_text, id=request_id, stage="reorchestrate"))
                                elif label == "answer":
                                    response = _stream_text
                            now_messages.append({"reasoning_content": _reasoning_content, "content": "response", "role": "assistant", "stage": "reorchestrate"})
                            latest_search_process = response['update_whole_step']
                            all_steps = step_by_step_process(latest_search_process)
                            queue.put(["bar", f"ðŸ”¨æ›´æ–°ä¸ºæµç¨‹ï¼š{latest_search_process}"])
                            context["online_steps"].append(f"ðŸ”¨æ›´æ–°ä¸ºæµç¨‹ï¼š{latest_search_process}")
                            api_queue.put(format_data(content=latest_search_process, id=request_id, stage="reorchestrate"))
                        else:
                            context['search_process'] = latest_search_process
                            break
                        

            elif now_stage == 'assistant':
                if len(all_evidences) == 0 or empty_search == True:
                    if LANGUAGE == "en":
                        all_evidences.append("There is no evidence, please reject to answer.")
                    else:
                        all_evidences.append("å½“å‰æ— è¯æ®å¥å­ï¼Œè¯·æ‹’ç»å›žç­”ã€‚")
                _reasoning_content = ""
                llm_answer = ""
                queue.put(["bar", "ðŸ–¥ï¸ æ™ºèƒ½ä½“ Â· ç»¼åˆèŽ·å–çš„ä¿¡æ¯"])
                context["online_steps"].append("ðŸ–¥ï¸ æ™ºèƒ½ä½“ Â· ç»¼åˆèŽ·å–çš„ä¿¡æ¯")
                queue.put(["placeholder_begin", None])
                context["online_steps"].append("")
                for label, _stream_text in assitant.run(context['question'], context['search_process'], all_evidences, code_evidences):
                    if label == "think":
                        _reasoning_content += _stream_text
                        queue.put(["placeholder_think_stream_markdown", convert_think_message_to_markdown(_reasoning_content)])
                        context["online_steps"][-1] = convert_think_message_to_markdown(_reasoning_content)
                        api_queue.put(format_data(reasoning_content=_stream_text, id=request_id, stage="answer with function call"))
                    elif label == "answer":
                        llm_answer += _stream_text
                        api_queue.put(format_data(content=_stream_text, id=request_id, stage="answer with function call"))
                    else:
                        llm_answer = _stream_text
                now_messages.append({"reasoning_content": _reasoning_content, "content": llm_answer, "role": "assistant", "stage": "answer with function call"})
                llm_no_ref_answer = remove_ref_tag(llm_answer)
                context['llm_answer'] = llm_answer
                if debug_verbose:
                    ref2md_answer = replace_ref_tag2md(llm_answer, context["online_url_lists"])
                    ref2md_answer = latex_render(ref2md_answer)
                    context["online_code_results"] = code_results_dict_list
                    context["online_answer"] = ref2md_answer
                now_messages.append(process_message(content = llm_no_ref_answer, role = "assistant", content_ref = llm_answer))

        meta_data = {
            "time_stamp":f"{get_real_time_str()}",
            "question": context['question'],
            "feedback": None,
            "messages" : now_messages,
            "tool_choice": "auto",
            "parallel_tool_calls": True,
            "tools": get_tool_list(use_tool_record)
        }
        # å°†æœ‰æ•ˆä¿¡æ¯æ”¶é›†èµ·æ¥
        if empty_search == False or empty_code == False:
            context["new_record"] = meta_data
            if meta_verbose:
                print("### json æœ€åŽå­˜å‚¨æ ¼å¼å¦‚ä¸‹ï¼š")
                print(json.dumps(meta_data, indent=4, ensure_ascii=False))

            if len(save_jsonl_path) > 0:
                with open(save_jsonl_path, 'a', encoding='utf-8') as f:
                    json_line = json.dumps(meta_data, ensure_ascii=False)
                    f.write(json_line + '\n')
    queue.put(["finish", None])
    api_queue.put("finish")
    return
    